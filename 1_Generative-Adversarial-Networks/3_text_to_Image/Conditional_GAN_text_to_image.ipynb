{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tarfile.open(r'<path>', 'r:gz') as tar:\n",
    "#     tar.list()\n",
    "#     tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load & Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load image labels\n",
    "        mat = scipy.io.loadmat(label_file)\n",
    "        self.labels = mat['labels'].flatten() - 1 # converts 1-based idx in matlab to 0-based idx in python\n",
    "        \n",
    "        # Load image file names\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        self.image_files.sort()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_file[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)), # resize image to (image_size x image_size)\n",
    "        transforms.ToTensor(), # convert arrays to tensors\n",
    "        transforms.Normalize([.5, .5, .5], [.5, .5, .5]), # normalize pixels (pixel - mean) / STD -> mean & SD for R,G,B is 0.5\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_dir = \"./data/jpg\"\n",
    "label_file = \"imagelabels.mat\"\n",
    "\n",
    "dataset = FlowerDataset(img_dir, label_file, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Define Conditional GAN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, label_dim, num_classes):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_dim) # create embedding layer to map label indices (102 different flowers)\n",
    "        self.model = nn.Sequential( # focus on linear layers only first\n",
    "            nn.Linear(latent_dim + label_dim, 256), # input to CondGenerator is noise & label, 256 output\n",
    "            nn.ReLU(), # introduce non-linearity\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, image_size * image_size), # output is 64 x 64 vector\n",
    "            nn.Tanh() # good practice to add at end to further normalize inputs\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        label_emb = self.label_embedding(labels) # map label indices (102 different flowers) by passing to embedding layer\n",
    "        generator_input = torch.cat((noise, label_emb), -1)  # concat tensors, -1 means combine by col to prep input\n",
    "        img = self.model(generator_input)\n",
    "        img = img.view(img.size(0), 3, image_size, image_size) # reshape to (batchsize, RGB, image_size, image_size) (64, 3, 64, 64)\n",
    "        return img\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, label_dim, num_classes):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_dim) # create embedding layer to map label indices (102 different flowers)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3 * image_size * image_size + label_dim, 512), # input to Discriminator is output from Generator + label_dim\n",
    "            nn.LeakyReLU(0.2), # to prevent vanishing gradient\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() # squeeze output between 0 and 1 - fake vs real label\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, labels): # propagate img from generator\n",
    "        img_flat = img.view(img.size(0), -1) # flatten from 3 x 64 x 64 to 64 x 64\n",
    "        label_emb = self.label_embedding(labels)\n",
    "        discriminator_input = torch.cat((img_flat, label_emb), -1) # concat img_flat & label_emb by col\n",
    "        validity = self.model(discriminator_input) # determine if img is valid or not\n",
    "        return validity\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Init & Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.0002\n",
    "latent_dim = 100 # noise vector dim\n",
    "label_dim = 50 # if equal num_classes, may overfit\n",
    "num_classes = 102\n",
    "n_epochs = 100\n",
    "\n",
    "# Model Init\n",
    "generator = ConditionalGenerator(latent_dim, label_dim, num_classes)\n",
    "discriminator = ConditionalDiscriminator(label_dim, num_classes)\n",
    "\n",
    "# Loss & Optim\n",
    "criterion = nn.BCELoss()\n",
    "optim_g = optim.Adam(generator.parameters(), lr=lr) \n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        label_emb = labels\n",
    "        real_imgs = imgs\n",
    "        \n",
    "        # create real & fake data ground truths\n",
    "        real_labels = torch.ones(imgs.size(0), 1) # real label means 1, imgs.size(0) is the batch_size\n",
    "        fake_labels = torch.zeros(imgs.size(0), 1) # fake label means 0\n",
    "    \n",
    "        # Train Discriminator\n",
    "        optim_d.zero_grad() # zero the gradient to prevent accumulation\n",
    "        real_output = discriminator(real_imgs, label_emb) # pass to forward(img, labels)\n",
    "        real_loss = criterion(real_output, real_labels)\n",
    "        \n",
    "        noise = torch.randn(imgs.size(0), latent_dim)\n",
    "        fake_imgs = generator(noise, label_emb).detach() # detach to prevent gradient update\n",
    "        fake_output = discriminator(fake_imgs, label_emb)\n",
    "        fake_loss = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss_total = real_loss + fake_loss\n",
    "        d_loss_total.backward() # backprop to calc grad\n",
    "        optim_d.step() # grad desc to update weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
