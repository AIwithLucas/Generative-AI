{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.133553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.061635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.175963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.302001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.082970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Temperature\n",
       "0    1     0.133553\n",
       "1    2     0.061635\n",
       "2    3     0.175963\n",
       "3    4     0.302001\n",
       "4    5     0.082970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/temperature_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Goal of generation:\n",
    "\n",
    "- perhaps look at first 30 - 60 days of temperature data\n",
    "- then from pattern, generate for next n days, following timeseries forecasting prediction to generate\n",
    "- i.e not one-step prediction but range-prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day            0\n",
       "Temperature    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = df['Temperature'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preprocessing (Time-series)\n",
    "\n",
    "- specify custom dataset (prep time-series data for training)\n",
    "- efficient data handling\n",
    "- compatible with data loaders\n",
    "- customized preprocessing\n",
    "  - converting data to tensors\n",
    "\n",
    "### Usage steps of Custom Dataset class\n",
    "\n",
    "1. pass temperature data to custom dataset for sequence to be created\n",
    "2. create dataloader with help of custom dataset\n",
    "3. feed dataloader to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30 # sequence length\n",
    "batch_size = 32 # No. of sequences to be trained per iteration\n",
    "\n",
    "# Dataset -> handling, managing data in structured way\n",
    "class TimeSeriesDataset(Dataset): \n",
    "    def __init__(self, data, sequence_length): # init custom dataset\n",
    "        self.data = data # to hold the temperature data\n",
    "        self.sequence_length = sequence_length # No. time steps to predict next value\n",
    "    \n",
    "    # return total No. of sequence that can be created, given no overlap\n",
    "    # eg: total data = 3, sequence length = 2, then we have 3 - 2 = 1 (1,2) or (2,3)\n",
    "    # +1 if overlap allowed\n",
    "    def __len__(self):  \n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    # retrieve specific sequence & corresponding target value\n",
    "    # i is start idx\n",
    "    def __getitem__(self, i):\n",
    "        x = self.data[i: i+self.sequence_length] # sequence data\n",
    "        y = self.data[i+self.sequence_length] # target\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(-1) # batch_size x sequence_length x input_dim\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset = TimeSeriesDataset(temp_data, T) # T = sequence_length\n",
    "temp_dataloader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False) # time-series should not shuffle data, order is impt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Generator / Discriminator Model Init\n",
    "\n",
    "- the design of the Neural network would be different for time-series data\n",
    "- time-series data = LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim, seq_len): # z_dim is the input dim\n",
    "        super(Generator, self).__init__() \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(z_dim, hidden_dim, num_layers=2, batch_first=True) # batch_first means (batchsize x seq_length x hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # adding linear layer + activation after can help increase non-linearity and make model capture more complex r/s\n",
    "    \n",
    "    def forward(self, x): # pass data through the layers created\n",
    "        h_0 = torch.zeros(2, x.size(0), self.hidden_dim) # to store all hidden states for LSTM (2 hidden layers, batchsize, hidden_dim)\n",
    "        c_0 = torch.zeros(2, x.size(0), self.hidden_dim) # to store all cell states for LSTM\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0)) # pass x & hidden + cell states thru layers, _ represents the final hidden and cell state output\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim): \n",
    "        super(Discriminator, self).__init__() \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True) # batch_first means (batchsize x seq_length x hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1) # output of discriminator is always 1, to tell if synthetic data is real or fake\n",
    "    \n",
    "    def forward(self, x): # pass data through the layers created\n",
    "        h_0 = torch.zeros(2, x.size(0), self.hidden_dim) # to store all hidden states for LSTM (2 hidden layers, batchsize, hidden_dim)\n",
    "        c_0 = torch.zeros(2, x.size(0), self.hidden_dim) # to store all cell states for LSTM\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0)) # _ represents the final hidden and cell state output\n",
    "        out = self.fc(lstm_out[:, -1,:]) # want (all batchsize, last_sequence, all hidden_dim) | evaluate only final_sequence for decision making of realness\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Init and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "z_dim = 1 # noise vector dim 1 for temperature\n",
    "output_dim = 1 # predicting single value temperature\n",
    "hidden_dim = 64 # no. of nodes in hidden layer for LSTM, more nodes = more complex capture\n",
    "num_epochs = 1000\n",
    "lr = 0.0001\n",
    "\n",
    "# Init Models\n",
    "generator = Generator(z_dim, hidden_dim, output_dim, seq_len=T)\n",
    "discriminator = Discriminator(z_dim, hidden_dim)\n",
    "\n",
    "# Define Loss & Optim\n",
    "criterion = nn.BCEWithLogitsLoss() # combines BCE with Sigmoid, makes it more numerically stable\n",
    "\n",
    "g_optim = optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Accumulate loss per epoch\n",
    "    g_loss_epoch = 0\n",
    "    d_loss_epoch = 0\n",
    "    \n",
    "    for real_batch, _ in temp_dataloader: # second element is usually label, but in this 1D temp data, no labels and just placeholder\n",
    "        batch_size = real_batch.size(0) # [32, 30, 1] - batchsize, sequence length, input dim (temp)\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1) # real_labels mean probability of 1, create batch_size x 1 col\n",
    "        fake_labels = torch.zeros(batch_size, 1) # fake_labels means probability of 0\n",
    "        # tensor([[1], [1]]) for real labels etc.\n",
    "\n",
    "        # Train Discriminator\n",
    "        d_optim.zero_grad() # zero gradient in discriminator to prevent accumulation from previous iteration before training\n",
    "        \n",
    "        real_output = discriminator(real_batch) # train real output first\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        noise = torch.randn(batch_size, T, z_dim) # T - sequence length\n",
    "        fake_data = generator(noise).detach() # generate fake outputs, detach to prevent gradients propagating thru generator\n",
    "        fake_output = discriminator(fake_data)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss_total = d_loss_real + d_loss_fake # want to minimize loss\n",
    "        d_loss_total.backward() # backprop to calc gradient\n",
    "        d_optim.step() #grad descent to update weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
